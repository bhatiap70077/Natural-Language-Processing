{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-1D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOT5ZsmuisQhMrYiLatppZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhatiap70077/Natural-Language-Processing/blob/main/Assignment_1D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "laoJsBf51qCh",
        "outputId": "f7c3b04f-400b-407d-d2d9-a0fcf6a66b25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.66-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 12.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85458 sha256=71ee9672fb9551c66fd2324db31b3a7f0ab32d1825b6d7679e88fb105b8d5307\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.1.66 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting nlpretext\n",
            "  Downloading nlpretext-1.1.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting spacy>=3.0.5\n",
            "  Downloading spacy-3.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 17.1 MB/s \n",
            "\u001b[?25hCollecting nltk<3.6,>=3.4.2\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 50.5 MB/s \n",
            "\u001b[?25hCollecting tornado>=6.0.3\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 65.5 MB/s \n",
            "\u001b[?25hCollecting fastparquet>=0.4.1\n",
            "  Downloading fastparquet-0.8.0-cp37-cp37m-manylinux2010_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>1.15.4 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.21.5)\n",
            "Collecting nlpaug>=1.0.1\n",
            "  Downloading nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
            "\u001b[K     |████████████████████████████████| 410 kB 53.1 MB/s \n",
            "\u001b[?25hCollecting emoji>=0.5.2\n",
            "  Downloading emoji-1.6.3.tar.gz (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting dask[complete]>=2021.5.0\n",
            "  Downloading dask-2022.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.2 MB/s \n",
            "\u001b[?25hCollecting phonenumbers>=8.10.12\n",
            "  Downloading phonenumbers-8.12.43-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 44.5 MB/s \n",
            "\u001b[?25hCollecting ftfy>=4.2.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting rich>=10.1.0\n",
            "  Downloading rich-11.2.0-py3-none-any.whl (217 kB)\n",
            "\u001b[K     |████████████████████████████████| 217 kB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex>=2019.8.19 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (2019.12.20)\n",
            "Collecting thinc>=8.0.4\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting mosestokenizer>=1.1.0\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "Collecting pillow>=8.2.1\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 42.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses>=0.0.13\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 57.6 MB/s \n",
            "\u001b[?25hCollecting typer[all]>=0.3.2\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting flashtext>=2.7\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (3.0.4)\n",
            "Requirement already satisfied: scikit-learn>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (1.3.5)\n",
            "Requirement already satisfied: importlib_metadata>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (4.11.0)\n",
            "Requirement already satisfied: pyarrow>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from nlpretext) (6.0.1)\n",
            "Collecting stop-words>=2018.7.23\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Collecting distributed>=2021.5.0\n",
            "  Downloading distributed-2022.2.0-py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 34.8 MB/s \n",
            "\u001b[?25hCollecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (21.3)\n",
            "Collecting pyyaml>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (0.11.2)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (1.3.0)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: bokeh>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (2.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from dask[complete]>=2021.5.0->nlpretext) (2.11.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (7.1.2)\n",
            "Collecting cloudpickle>=1.1.1\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (1.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (57.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (2.0.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (5.4.8)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (2.4.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2021.5.0->nlpretext) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=2.1.1->dask[complete]>=2021.5.0->nlpretext) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=2.1.1->dask[complete]>=2021.5.0->nlpretext) (2.8.2)\n",
            "Collecting cramjam>=2.3.0\n",
            "  Downloading cramjam-2.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 44.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy>=4.2.0->nlpretext) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=1.6.0->nlpretext) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->dask[complete]>=2021.5.0->nlpretext) (2.0.1)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from mosestokenizer>=1.1.0->nlpretext) (0.6.2)\n",
            "Collecting openfile\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "Collecting toolwrapper\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from nlpaug>=1.0.1->nlpretext) (2.23.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<3.6,>=3.4.2->nlpretext) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk<3.6,>=3.4.2->nlpretext) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->dask[complete]>=2021.5.0->nlpretext) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->nlpretext) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh>=2.1.1->dask[complete]>=2021.5.0->nlpretext) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug>=1.0.1->nlpretext) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug>=1.0.1->nlpretext) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->nlpaug>=1.0.1->nlpretext) (2021.10.8)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.1.0->nlpretext) (2.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->nlpretext) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.2->nlpretext) (3.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (1.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (2.0.6)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 37.1 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (0.4.1)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (0.9.0)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0.5->nlpretext) (3.0.6)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0.5->nlpretext) (5.2.1)\n",
            "Collecting shellingham<2.0.0,>=1.3.0\n",
            "  Downloading shellingham-1.4.0-py2.py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2021.5.0->nlpretext) (1.0.1)\n",
            "Building wheels for collected packages: emoji, flashtext, mosestokenizer, nltk, stop-words, toolwrapper, uctools\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.6.3-py3-none-any.whl size=170298 sha256=346a39f9165f7e8f9260c48db02d594e79942740ec633a80a1882a03345b657d\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/8b/d7/ad579fbef83c287215c0caab60fb0ae0f30c4d7ce5f580eade\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=30c6f393bdc3aefb8c7cca18e3fad9cdbd7859cc7d3a220b8cb8289db562534c\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49189 sha256=adb406970ba6a549bfb11d1d2afd2247531e78851f0b9f8b703c5316377ea43e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/35/f7/af1258779a0b890abc3c79481460c597cb1f3659d0603cfb9d\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434690 sha256=a0809c34f3b8dd878494743cb715c17880ad9a164f5cf10a1fb95ee3c396f630\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=72f01a243f9af9ff54a592936ef8ecba4511760d3322d2c88616372308c29516\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3353 sha256=069009686c9ba1a03ae140d6de108211d574c5e5e87defc5106857b9c81824a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4f/33/54741ffe08e38ececb1d28068a153729b4fe820bafa0a0691f\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6161 sha256=c99dd6dde6f2ea2aecce1b3da045d511a15a1d7f0f7b4cb28bc8df37c72ab352\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/44/e9/914cf8fa71f0141f9314f862538d1218fcf2b94542a0fb7d35\n",
            "Successfully built emoji flashtext mosestokenizer nltk stop-words toolwrapper uctools\n",
            "Installing collected packages: locket, pyyaml, partd, fsspec, cloudpickle, catalogue, typer, tornado, srsly, pydantic, pillow, dask, uctools, toolwrapper, thinc, spacy-loggers, spacy-legacy, shellingham, pathy, openfile, langcodes, distributed, cramjam, commonmark, colorama, stop-words, spacy, sacremoses, rich, phonenumbers, nltk, nlpaug, mosestokenizer, ftfy, flashtext, fastparquet, emoji, nlpretext\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed catalogue-2.0.6 cloudpickle-2.0.0 colorama-0.4.4 commonmark-0.9.1 cramjam-2.5.0 dask-2022.2.0 distributed-2022.2.0 emoji-1.6.3 fastparquet-0.8.0 flashtext-2.7 fsspec-2022.1.0 ftfy-6.1.1 langcodes-3.3.0 locket-0.2.1 mosestokenizer-1.2.1 nlpaug-1.1.10 nlpretext-1.1.0 nltk-3.5 openfile-0.0.7 partd-1.2.0 pathy-0.6.1 phonenumbers-8.12.43 pillow-9.0.1 pydantic-1.8.2 pyyaml-6.0 rich-11.2.0 sacremoses-0.0.47 shellingham-1.4.0 spacy-3.2.2 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 stop-words-2018.7.23 thinc-8.0.13 toolwrapper-2.1.0 tornado-6.1 typer-0.4.0 uctools-1.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cloudpickle",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install contractions\n",
        "!pip install nlpretext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "import re\n",
        "import contractions\n",
        "from nlpretext import Preprocessor\n",
        "import nltk\n",
        "import pandas\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "text=\"\"\"This movie made it into one of my top 10 most awful movies. Horrible. \n",
        "        I don’t care if it makes 1 million, 10 M , or 100. \n",
        "        There wasn't a continuous minute where there wasn't a fight with one monster or another. \n",
        "        There was no chance for any character development, they were too busy running from one sword fight to another. \n",
        "        I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them). \n",
        "        If you disagree with me, you can send your thoughts to idonotcare@leavemealone.com\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRCYQwcn3RAQ",
        "outputId": "99282811-7d9c-4fa6-c4a3-b396d7978808"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "logger_info = logging.getLogger()"
      ],
      "metadata": {
        "id": "xEJwFDJQ3cdn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Text_Preprocess:\n",
        "    \"\"\"\n",
        "    This class(Text_Ptreprocess) object contains functions to pre-process the data. \n",
        "    Functions inside classes : \n",
        "    ------------------------------\n",
        "    1) exp_contractions()--> to expand contractions\n",
        "    2) spec_char()-->        to remove spcecial characters and extra spaces\n",
        "    3) simp_dig_email()----> Simplifying the digits and emails.\n",
        "    4) tokenization()-->     to breakdown the text into tokens.\n",
        "    5) Stop words()--------->to remove the stop words.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, text):\n",
        "        self.text = text\n",
        "\n",
        "\n",
        "    def exp_contractions(self,text:str)->str:\n",
        "        '''\n",
        "        This method expands the contractions in the given text\n",
        "            Input arguments : text (Here the input is the text and this function expands the contractions inside the text \n",
        "            ---------------         For Ex: doesn't changes to does not)\n",
        "            Returns: We get the expanded text\n",
        "            --------\n",
        "            '''\n",
        "            # creating an empty list\n",
        "        expanded_words = []    \n",
        "        for word in text.split():\n",
        "            ''''''\n",
        "            # using contractions.fix to expand the shotened words\n",
        "            expanded_words.append(contractions.fix(word))   \n",
        "\n",
        "        expanded_text = ' '.join(expanded_words)\n",
        "        return expanded_text\n",
        "    '''\n",
        "    Process finished with exit code 0\n",
        "    '''\n",
        "    \n",
        "    def spec_char(self,text:str)->str:\n",
        "        '''This method removes the special characters along with extra spaces\n",
        "        Input arguments: text ( In this function all the special characters will be removed and extra spaces will be cleared )\n",
        "        ---------------     \n",
        "        Returns: The final text will have cleared special characters.'''\n",
        "        preprocessor = Preprocessor()\n",
        "        Formatted_Text = re.sub(r\"[^A-Za-z]+\", ' ', text)\n",
        "        '''The regular expression written above returns a match if the string contains characters except specified.'''\n",
        "        spec_text = preprocessor.run(Formatted_Text)\n",
        "        return spec_text\n",
        "        '''It returns the text clearing special characters.'''\n",
        "   \n",
        "    def simp_dig_email(self,text:str)->str:\n",
        "        ''' \n",
        "        This method simplifies the digits and emails using regular expressions.\n",
        "              Input arguments: text (In this function the digits and email will be cleared by using certain patterns)\n",
        "              ---------------     \n",
        "              Returns:  We get simplified text.\n",
        "        '''\n",
        "        preprocessor = Preprocessor()\n",
        "        Formatted_Text = re.sub(r\"-@]+\", ' ', text)\n",
        "        simp_text = preprocessor.run(Formatted_Text)\n",
        "        return simp_text\n",
        "    \n",
        "    def tokenization(self,text:str)->list:\n",
        "        ''' In this function the whole text will be divided into tokens.\n",
        "                By tokenization we can transform indivisible assets into tokens'''\n",
        "        tokenized_text = nltk.tokenize.word_tokenize(text)\n",
        "        return tokenized_text\n",
        "    \n",
        "    def stopwords(self,tokens:list, method:str)->list:\n",
        "        ''' \n",
        "        Stopwords are a set of commonly used words such as is, the, are etc.. \n",
        "        Input\n",
        "        -----\n",
        "        text file as string\n",
        "        \n",
        "        Returns\n",
        "        ------\n",
        "        list of words without stopwords\n",
        "        '''\n",
        "        nltk.download('stopwords')\n",
        "        list_of_stop_words = stopwords.words('english')\n",
        "\n",
        "        list_of_stop_words[1:10]\n",
        "\n",
        "        token_without_stopwords = [i for i in tokens if i not in list_of_stop_words]\n",
        "        return token_without_stopwords\n",
        "    \n",
        "    \n",
        "    \n",
        "    def stem_or_lem(self, tokens:list,method:str)->list:\n",
        "        '''\n",
        "        This function performs stemming and lemmatization on the list of words.\n",
        "        paramteter\n",
        "        ----------\n",
        "        input: list of tokens\n",
        "        \n",
        "        returns\n",
        "        -----\n",
        "        list of words after stemming and lemmatization.\n",
        "        '''\n",
        "   #instance of PorterStemmer \n",
        "        ps = PorterStemmer()\n",
        "        stemmed=[]\n",
        "        lemmed=[]\n",
        "        if method =='stemm':\n",
        "            for w in tokens:\n",
        "                rootWord=ps.stem(w)\n",
        "                stemmed.append(rootWord)\n",
        "            return stemmed\n",
        "        elif method =='lemm':\n",
        "            wordnet_lemmatizer = WordNetLemmatizer()\n",
        "            for w in tokens:\n",
        "                lemm = wordnet_lemmatizer.lemmatize(w)\n",
        "                lemmed.append(lemm)\n",
        "            return lemmed\n",
        "        else:\n",
        "            return tokens\n",
        "\n",
        "    \n",
        " \n",
        "    def get_preprocessed_text(self):\n",
        "        con_text = self.exp_contractions(self.text)\n",
        "        special_text = self.spec_char(con_text)\n",
        "        simplify_text = self.simp_dig_email(special_text)\n",
        "        token_text = self.tokenization(simplify_text)\n",
        "        stopword_text = self.stopwords(token_text, 'english')\n",
        "        stemmed =self.stem_or_lem(stopword_text,'stemm')\n",
        "        lemmed =self.stem_or_lem(stemmed,'lemmed')\n",
        "        return lemmed\n",
        "\n",
        "\n",
        "    def bags_of_words(self, corpus_texts):\n",
        "        \"\"\"\n",
        "        This method will apply the Bags of words to the list of sentences\n",
        "        in the arguments and return the vectors.\n",
        "\n",
        "        :param sentences: List of sentences where we need to apply Bags of Words\n",
        "        :return sentences_vectors: Numpy array of vectors\n",
        "        \"\"\"\n",
        "        wordfreq = {}\n",
        "        for sentence in corpus_texts:\n",
        "            # sent_processing = ProcessingText(sentence)\n",
        "            self.text = sentence\n",
        "            processed_sent = self.get_preprocessed_text()\n",
        "            for token in processed_sent:\n",
        "                if token not in wordfreq.keys():\n",
        "                    wordfreq[token] = 1\n",
        "                else:\n",
        "                    wordfreq[token] += 1\n",
        "                    \n",
        "                    \n",
        "        sentence_vectors = []\n",
        "        for sentence in corpus_texts:\n",
        "            # sent_processing = ProcessingText(sentence)\n",
        "            self.text = sentence\n",
        "            processed_sent = self.get_preprocessed_text()\n",
        "            sent_vec = []\n",
        "            print(processed_sent)\n",
        "            for token in wordfreq:\n",
        "                if token in processed_sent:\n",
        "                    print(token, processed_sent)\n",
        "                    \n",
        "                    sent_vec.append(1)\n",
        "                    print(sent_vec)\n",
        "                else:\n",
        "                    sent_vec.append(0)\n",
        "            sentence_vectors.append(sent_vec)\n",
        "\n",
        "\n",
        "        return (sentence_vectors)   \n",
        "\n",
        "    def tf_idf(self, corpus_texts):\n",
        "        \"\"\"\n",
        "        This method will apply the TFIDF to the list of sentences\n",
        "        in the arguments and return the vectors.\n",
        "\n",
        "        :param sentences: List of sentences where we need to apply TFIDF\n",
        "        :return sentences_vectors: Numpy array of vectors\n",
        "        \"\"\"\n",
        "        wordfreq = {}\n",
        "        for sentence in corpus_texts:\n",
        "            # sent_processing = ProcessingText(sentence)\n",
        "            self.text = sentence\n",
        "            processed_sent = self.get_preprocessed_text()\n",
        "            for token in processed_sent:\n",
        "                if token not in wordfreq.keys():\n",
        "                    wordfreq[token] = 1\n",
        "                else:\n",
        "                    wordfreq[token] += 1\n",
        "        vocab = wordfreq\n",
        "        \n",
        "        termDict={}\n",
        "        docsTFMat = np.zeros((len(corpus_texts),len(vocab)))\n",
        "        docsIdfMat = np.zeros((len(vocab),len(corpus_texts)))\n",
        "\n",
        "        docTermDf = pd.DataFrame(docsTFMat ,columns=sorted(vocab.keys()))\n",
        "        docCount=0\n",
        "        for doc in corpus_texts:\n",
        "            self.text = doc\n",
        "            doc = self.get_preprocessed_text()\n",
        "            \n",
        "            for word in doc:\n",
        "                if (word in vocab.keys()):\n",
        "                    \n",
        "                    docTermDf[word][docCount] = docTermDf[word][docCount] +1\n",
        "\n",
        "            docCount = docCount +1\n",
        "\n",
        "\n",
        "        #Computed idf for each word in vocab\n",
        "        idfDict={}\n",
        "\n",
        "        for column in docTermDf.columns:\n",
        "            idfDict[column]= np.log((len(corpus_texts) +1 )/(1+ (docTermDf[column] != 0).sum()))+1\n",
        "\n",
        "        #compute tf.idf matrix\n",
        "        docsTfIdfMat = np.zeros((len(corpus_texts),len(vocab)))\n",
        "        docTfIdfDf = pd.DataFrame(docsTfIdfMat ,columns=sorted(vocab.keys()))\n",
        "\n",
        "\n",
        "\n",
        "        docCount = 0\n",
        "        for doc in corpus_texts:\n",
        "            for key in idfDict.keys():\n",
        "                docTfIdfDf[key][docCount] = docTermDf[key][docCount] * idfDict[key]\n",
        "            docCount = docCount +1\n",
        "        return docTfIdfDf \n",
        "    "
      ],
      "metadata": {
        "id": "zOT8hwAo3cgT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Class Object\n",
        "\"\"\"\n",
        "Main Class\n",
        "\"\"\"\n",
        "Class_obj = Text_Preprocess(text)\n",
        "Class_obj.get_preprocessed_text()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU37wCiJ3cjV",
        "outputId": "266bcceb-4adf-4a09-e5cd-cf069afa7f47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thi',\n",
              " 'movi',\n",
              " 'made',\n",
              " 'one',\n",
              " 'top',\n",
              " 'aw',\n",
              " 'movi',\n",
              " 'horribl',\n",
              " 'I',\n",
              " 'care',\n",
              " 'make',\n",
              " 'million',\n",
              " 'M',\n",
              " 'there',\n",
              " 'continu',\n",
              " 'minut',\n",
              " 'fight',\n",
              " 'one',\n",
              " 'monster',\n",
              " 'anoth',\n",
              " 'there',\n",
              " 'chanc',\n",
              " 'charact',\n",
              " 'develop',\n",
              " 'busi',\n",
              " 'run',\n",
              " 'one',\n",
              " 'sword',\n",
              " 'fight',\n",
              " 'anoth',\n",
              " 'I',\n",
              " 'emot',\n",
              " 'attach',\n",
              " 'except',\n",
              " 'big',\n",
              " 'bad',\n",
              " 'machin',\n",
              " 'want',\n",
              " 'destroy',\n",
              " 'If',\n",
              " 'disagre',\n",
              " 'send',\n",
              " 'thought',\n",
              " 'idonotcar',\n",
              " 'leavemealon',\n",
              " 'com']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "sent_1 = 'This is a good movie.'\n",
        "sent_2 = 'It is a good movie, but you know good is relative.'\n",
        "sent_3 = 'Movie is fun to watch.'\n",
        "sent_4 = 'I had a good relaxing time.'\n",
        "sent_5 = 'The whole cinema experience was good.'\n",
        "sent_6 = 'This is a good cinema.' \n",
        "\n",
        "corpus_text = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
        "processed = (Class_obj.bags_of_words(corpus_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqjvx0e83clt",
        "outputId": "7e4b59aa-0518-4ab7-9361-04bce68dc07a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'good', 'movi']\n",
            "thi ['thi', 'good', 'movi']\n",
            "[1]\n",
            "good ['thi', 'good', 'movi']\n",
            "[1, 1]\n",
            "movi ['thi', 'good', 'movi']\n",
            "[1, 1, 1]\n",
            "['It', 'good', 'movi', 'know', 'good', 'rel']\n",
            "good ['It', 'good', 'movi', 'know', 'good', 'rel']\n",
            "[0, 1]\n",
            "movi ['It', 'good', 'movi', 'know', 'good', 'rel']\n",
            "[0, 1, 1]\n",
            "It ['It', 'good', 'movi', 'know', 'good', 'rel']\n",
            "[0, 1, 1, 1]\n",
            "know ['It', 'good', 'movi', 'know', 'good', 'rel']\n",
            "[0, 1, 1, 1, 1]\n",
            "rel ['It', 'good', 'movi', 'know', 'good', 'rel']\n",
            "[0, 1, 1, 1, 1, 1]\n",
            "['movi', 'fun', 'watch']\n",
            "movi ['movi', 'fun', 'watch']\n",
            "[0, 0, 1]\n",
            "fun ['movi', 'fun', 'watch']\n",
            "[0, 0, 1, 0, 0, 0, 1]\n",
            "watch ['movi', 'fun', 'watch']\n",
            "[0, 0, 1, 0, 0, 0, 1, 1]\n",
            "['I', 'good', 'relax', 'time']\n",
            "good ['I', 'good', 'relax', 'time']\n",
            "[0, 1]\n",
            "I ['I', 'good', 'relax', 'time']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "relax ['I', 'good', 'relax', 'time']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "time ['I', 'good', 'relax', 'time']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "['the', 'whole', 'cinema', 'experi', 'good']\n",
            "good ['the', 'whole', 'cinema', 'experi', 'good']\n",
            "[0, 1]\n",
            "the ['the', 'whole', 'cinema', 'experi', 'good']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
            "whole ['the', 'whole', 'cinema', 'experi', 'good']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
            "cinema ['the', 'whole', 'cinema', 'experi', 'good']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
            "experi ['the', 'whole', 'cinema', 'experi', 'good']\n",
            "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "['thi', 'good', 'cinema']\n",
            "thi ['thi', 'good', 'cinema']\n",
            "[1]\n",
            "good ['thi', 'good', 'cinema']\n",
            "[1, 1]\n",
            "cinema ['thi', 'good', 'cinema']\n",
            "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in processed:\n",
        "    logger_info.info(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bms_eL_V3coa",
        "outputId": "faabd7bc-ecdf-4b58-e1d4-a37fc2e9a08c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:root:[0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:root:[0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "INFO:root:[0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n",
            "INFO:root:[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "INFO:root:[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "sent_1 = 'This is a good movie.'\n",
        "sent_2 = 'It is a good movie, but you know good is relative.'\n",
        "sent_3 = 'Movie is fun to watch.'\n",
        "sent_4 = 'I had a good relaxing time.'\n",
        "sent_5 = 'The whole cinema experience was good.'\n",
        "sent_6 = 'This is a good cinema.'\n",
        "\n",
        "corpus_text = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
        "processed = (Class_obj.tf_idf(corpus_text))\n",
        "\n",
        "processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "LEhDlTpu3crV",
        "outputId": "38863987-c5a0-4947-d8e2-d5c5dbcd7440"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6e76139-5574-4a57-94f8-a3fbfd77f37d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I</th>\n",
              "      <th>It</th>\n",
              "      <th>cinema</th>\n",
              "      <th>experi</th>\n",
              "      <th>fun</th>\n",
              "      <th>good</th>\n",
              "      <th>know</th>\n",
              "      <th>movi</th>\n",
              "      <th>rel</th>\n",
              "      <th>relax</th>\n",
              "      <th>the</th>\n",
              "      <th>thi</th>\n",
              "      <th>time</th>\n",
              "      <th>watch</th>\n",
              "      <th>whole</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.154151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.559616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.847298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.308301</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>1.559616</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.559616</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.154151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.847298</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.154151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.252763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.847298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.154151</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.847298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6e76139-5574-4a57-94f8-a3fbfd77f37d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6e76139-5574-4a57-94f8-a3fbfd77f37d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6e76139-5574-4a57-94f8-a3fbfd77f37d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          I        It    cinema  ...      time     watch     whole\n",
              "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "1  0.000000  2.252763  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2  0.000000  0.000000  0.000000  ...  0.000000  2.252763  0.000000\n",
              "3  2.252763  0.000000  0.000000  ...  2.252763  0.000000  0.000000\n",
              "4  0.000000  0.000000  1.847298  ...  0.000000  0.000000  2.252763\n",
              "5  0.000000  0.000000  1.847298  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[6 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PvaDhvA5D6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}